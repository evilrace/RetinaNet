{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model import LabelEncoder, Anchor\n",
    "from utils import data_preprocess, data_generator\n",
    "import numpy as np \n",
    "import tensorflow_datasets as tfds\n",
    "import model.Model as Model\n",
    "import model.Loss as Loss\n",
    "import os\n",
    "\n",
    "data_dir = r'F:\\dataset\\kitti\\data_object_image_2\\training'\n",
    "model_dir = r'retinanet\\\\'\n",
    "label_encoder = LabelEncoder.LabelEncoder()\n",
    "\n",
    "num_classes = 8\n",
    "batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data list\n",
    "img_list = tf.io.gfile.glob(f'{data_dir}\\image_2\\*.png')\n",
    "label_list = tf.io.gfile.glob(f'{data_dir}\\label_2\\*.txt')\n",
    "img_list.sort()\n",
    "label_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, labels, classes = data_preprocess.prepare_data(img_list[0],label_list[0])\n",
    "# ax = data_preprocess.visualize_detections(img, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "learning_rate = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values = learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_generator.get_dataset(img_list, label_list)\n",
    "dataset = dataset.shuffle(32)\n",
    "\n",
    "dataset = dataset.padded_batch(\n",
    "    batch_size = batch_size, padding_values = (0.0, 1e-8, -1.0), drop_remainder=True\n",
    ")\n",
    "dataset = dataset.map(label_encoder.encode_batch)\n",
    "dataset = dataset.apply(tf.data.experimental.ignore_errors())\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset = dataset.take(100)\n",
    "val_dataset = dataset.skip(100).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(iter(train_dataset))\n",
    "# print(sample[0][0].shape)\n",
    "# ax = data_preprocess.visualize_detections(sample[0][0], sample[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_backbone = Model.get_backbone()\n",
    "loss_fn = Loss.RetinaNetLoss(num_classes)\n",
    "model = Model.RetinaNet(num_classes, backbone = resnet50_backbone)\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "model.compile(loss=loss_fn, optimizer= optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    100/Unknown - 108s 889ms/step - loss: 0.7139\n",
      "Epoch 1: saving model to retinanet\\weights_epoch_1\n",
      "100/100 [==============================] - 198s 2s/step - loss: 0.7139 - val_loss: 0.4141\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7036\n",
      "Epoch 2: saving model to retinanet\\weights_epoch_2\n",
      "100/100 [==============================] - 130s 1s/step - loss: 0.7036 - val_loss: 0.4199\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6726\n",
      "Epoch 3: saving model to retinanet\\weights_epoch_3\n",
      "100/100 [==============================] - 128s 1s/step - loss: 0.6726 - val_loss: 0.4101\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6647\n",
      "Epoch 4: saving model to retinanet\\weights_epoch_4\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.6647 - val_loss: 0.3956\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6876\n",
      "Epoch 5: saving model to retinanet\\weights_epoch_5\n",
      "100/100 [==============================] - 159s 2s/step - loss: 0.6876 - val_loss: 0.3813\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6644\n",
      "Epoch 6: saving model to retinanet\\weights_epoch_6\n",
      "100/100 [==============================] - 136s 1s/step - loss: 0.6644 - val_loss: 0.3582\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5067\n",
      "Epoch 7: saving model to retinanet\\weights_epoch_7\n",
      "100/100 [==============================] - 127s 1s/step - loss: 0.5067 - val_loss: 0.2932\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5659\n",
      "Epoch 8: saving model to retinanet\\weights_epoch_8\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.5659 - val_loss: 0.4262\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5977\n",
      "Epoch 9: saving model to retinanet\\weights_epoch_9\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.5977 - val_loss: 0.3541\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5996\n",
      "Epoch 10: saving model to retinanet\\weights_epoch_10\n",
      "100/100 [==============================] - 122s 1s/step - loss: 0.5996 - val_loss: 1.6636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b3f2b394f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    epochs=10,\n",
    "    callbacks = callbacks_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Model import DecodePredictions\n",
    "weights_dir = 'retinanet'\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "model.load_weights(latest_checkpoint)\n",
    "\n",
    "image = tf.keras.Input(shape=[None, None, 3], name='image')\n",
    "predictions = model(image, training=False)\n",
    "detections = DecodePredictions(confidence_threshold = 0.5, num_classes = num_classes)(image, predictions)\n",
    "inference_model = tf.keras.Model(inputs= image, outputs=detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = next(iter(val_dataset))\n",
    "inference_model.predict(sample[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5f227904729d2e34638b12d6189826106a201c215150695c35e0f334bf03903"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
